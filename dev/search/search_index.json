{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Introduction","text":"<p>Argilla is a collaboration platform for AI engineers and domain experts that require high-quality outputs, full data ownership, and overall efficiency.</p> <ul> <li> <p> Get started in 5 minutes</p> <p>Install <code>argilla</code> with <code>pip</code> and deploy a <code>Docker</code> for free on Hugging Face to get up and running in minutes.</p> <p> Getting started</p> </li> <li> <p> Installation</p> <p>Learn how to <code>pip</code> install Argilla and configure the server and UI using <code>Docker</code>, <code>k8s</code> and cloud proivders.</p> <p> Installation</p> </li> <li> <p> How to guides</p> <p>Get familiar with common workflows for Argilla. From managing <code>Users</code>, <code>Workspaces</code>. <code>Datasets</code> and <code>Records</code> to fine-tuning a model.</p> <p> How to guide</p> </li> <li> <p> Tutorials</p> <p>Discover cool applied examples of Argilla working on techniques like RAG with <code>haystack</code>, fine-tuning <code>transformers</code>,  few-shot textcat with <code>setfit</code> and more.</p> <p> Tutorials</p> </li> <li> <p> UI demo</p> <p>If you just want to get started, we recommend our <code>UI demo</code> which contains non-technical human readable examples to showcase the diversity of Argilla.</p> <p> UI demo</p> </li> <li> <p> API Reference</p> <p>Get familiar with the codebase of our <code>Python SDK</code> and <code>FastAPI server</code>.</p> <p> API Reference</p> </li> </ul>"},{"location":"#why-use-argilla","title":"Why use Argilla?","text":"<p>Whether you are working on monitoring and improving complex generative tasks involving LLM pipelines with RAG, or you are working on a predictive task for things like AB-testing of span- and text-classification models. Our versatile platform helps you ensure your data work pays off.</p>"},{"location":"#improve-your-ai-output-quality-through-data-quality","title":"Improve your AI output quality through data quality","text":"<p>Compute is expensive and output quality is important. We help you focus on data, which tackles the root cause of both of these problems at once. Argilla helps you to achieve and keep high-quality standards for your data. This means you can improve the quality of your AI output.</p>"},{"location":"#take-control-of-your-data-and-models","title":"Take control of your data and models","text":"<p>Most AI platforms are black boxes. Argilla is different. We believe that you should be the owner of both your data and your models. That's why we provide you with all the tools your team needs to manage your data and models in a way that suits you best.</p>"},{"location":"#improve-efficiency-by-quickly-iterating-on-the-right-data-and-models","title":"Improve efficiency by quickly iterating on the right data and models","text":"<p>Gathering data is a time-consuming process. Argilla helps by providing a platform that allows you to interact with your data in a more engaging way. This means you can quickly and easily label your data with filters, AI feedback suggestions and semantic search. So you can focus on training your models and monitoring their performance.</p>"},{"location":"#what-do-people-build-with-argilla","title":"What do people build with Argilla?","text":""},{"location":"#datasets-and-models","title":"Datasets and models","text":"<p>Argilla is a tool that can be used to achieve and keep high-quality data standards with a focus on NLP and LLMs. Our community uses Argilla to create amazing open-source datasets and models, and we love contributions to open-source ourselves too.</p> <ul> <li>Our cleaned UltraFeedback dataset and the Notus and Notux models, where we improved benchmark and empirical human judgment for the Mistral and Mixtral models with cleaner data using human feedback.</li> <li>Our distilabeled Intel Orca DPO dataset and the improved OpenHermes model, show how we improve model performance by filtering out 50% of the original dataset through human and AI feedback.</li> </ul>"},{"location":"#projects-and-pipelines","title":"Projects and pipelines","text":"<p>AI teams from companies like the Red Cross, Loris.ai and Prolific use Argilla to improve the quality and efficiency of AI projects. They shared their experiences in our AI community meetup.</p> <ul> <li>AI for good: the Red Cross presentation showcases how their experts and AI team collaborate by classifying and redirecting requests from refugees of the Ukrainian crisis to streamline the support processes of the Red Cross.</li> <li>Customer support: during the Loris meetup they showed how their AI team uses unsupervised and few-shot contrastive learning to help them quickly validate and gain labelled samples for a huge amount of multi-label classifiers.</li> <li>Research studies: the showcase from Prolific announced their integration with our platform. They use it to actively distribute data collection projects among their annotating workforce. This allows them to quickly and efficiently collect high-quality data for their research studies.</li> </ul>"},{"location":"_source/tutorials/add-suggestions-and-responses-005/","title":"Add suggestions and responses 005","text":"<p>First, let's install our dependencies and import the necessary libraries:</p> <pre><code>!pip install \"argilla-python\"\n!pip install datasets transformers\n</code></pre> <pre><code>import argilla_sdk as rg\nfrom datasets import load_dataset\nfrom transformers import pipeline\nfrom datetime import datetime\n</code></pre> <p>In order to run this notebook we will need some credentials to push and load datasets from <code>Argilla</code> and <code>\ud83e\udd17 Hub</code>, let's set them in the following cell:</p> <p>Log in to argilla:</p> <pre><code>client = rg.Argilla()\n</code></pre> <pre><code>dataset = client.datasets.get(name=\"end2end_textclassification\", workspace=\"argilla\")\n</code></pre> <p>Let us briefly examine what our dataset looks like. It is a dataset that consists of data items with the field <code>text</code> that is yet to be annotated.</p> <pre><code>dataset[0].fields\n</code></pre> <pre>\n<code>{'text': \"Wall St. Bears Claw Back Into the Black (Reuters) Reuters - Short-sellers, Wall Street's dwindling\\\\band of ultra-cynics, are seeing green again.\"}</code>\n</pre> <p>As we will be using the gold labels in the original dataset as the responses, we can pull the original dataset from HuggingFace Hub. We can do this by using the <code>load_dataset</code> method.</p> <pre><code>dataset_org = load_dataset(\"ag_news\", split=\"train[:1000]\")\n</code></pre> <p>The labels in the original dataset are in the form of integers while we need to present them to the annotators in the form of strings. Therefore, we will create a dictionary that maps the integer labels to their string counterparts.</p> <pre><code>for index, record in enumerate(dataset.records):\n    record.responses.create(\n        name=\"label\",\n        value=dataset_org[index][\"label\"],\n    )\n</code></pre> <pre><code>model_name = \"cointegrated/rubert-tiny-bilingual-nli\"\nclassifier = pipeline(\"zero-shot-classification\", model=model_name)\ncandidate_labels = [\"World\", \"Sports\", \"Business\", \"Sci/Tech\"]\n</code></pre> <p>We can iterate over the data items and add the suggestions for each item by classifying the text with the model at the same time. Additionally, we would like to add the model as the <code>agent</code> of the suggestions for future reference.</p> <pre><code>for record in dataset.records:\n    record.suggestions.create(\n        name=\"label\", value=classifier(record.fields[\"text\"], candidate_labels)[\"labels\"][0], agent=model_name\n    )\n</code></pre> <pre><code>dataset.publish()\n</code></pre> <p>Let us go to Argilla and look at the dataset we created. When you first open the dataset, you should the message \"You have no pending records\" as we have uploaded all the responses and there is no pending record left. As seen below, if you go to the <code>Submitted</code> tab, you can see that all the records are submitted with the exact labels we have uploaded above.</p> <p></p> <p>As all the records are submitted, we no longer see the suggestion for each one of the records. To demonstrate how they are seen, you can see the Argilla UI below where the record is not submitted yet and the suggestion (<code>Sports</code> in this case) is shown to the annotator.</p> <p></p> <pre><code>from huggingface_hub import login\n\nhf_token = \"YOUR_HF_TOKEN\"\nlogin(token=hf_token)\n</code></pre> <p>We only need to call the <code>to_datasets</code> method to convert the dataset to datasets format.</p> <pre><code>ds = dataset.to_datasets()\nds.push_to_hub(\"argilla/end2end_textclassification\", use_auth_token=hf_token)\n</code></pre>"},{"location":"_source/tutorials/add-suggestions-and-responses-005/#add-responses-and-suggestions-to-feedbackdataset","title":"Add <code>Responses</code> and <code>Suggestions</code> to <code>FeedbackDataset</code>","text":"<p>In this part of the end-to-end tutorial series, we will see how we can update the records of our dataset with the responses and suggestions. You can refer to previous tutorials for creating the dataset, configuring the users and workspaces or adding metadata. Feel free to check out the practical guides page for more in-depth information.</p> <p>In Argilla, <code>responses</code> are the answers that the annotators give to the questions that we ask them. If we have a dataset that has been annotated already, we can add these gold responses to our dataset as responses. This comes in handy in that we will not have to annotate the dataset again. On the other hand, <code>suggestions</code> are the model predictions that we show to our annotators in the UI during the annotation process. This way, the annotation process will become much faster and easier for the annotators.</p> <p></p>"},{"location":"_source/tutorials/add-suggestions-and-responses-005/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Pull the Dataset</li> <li>From Argilla</li> <li>From HuggingFace Hub</li> <li>Pull the Original Dataset</li> <li>Add Responses</li> <li>Add Suggestions</li> <li>Push the Dataset</li> <li>To Argilla</li> <li>To HuggingFace Hub</li> <li>Conclusion</li> </ol>"},{"location":"_source/tutorials/add-suggestions-and-responses-005/#running-argilla","title":"Running Argilla","text":"<p>For this tutorial, you will need to have an Argilla server running. There are two main options for deploying and running Argilla:</p> <p>Deploy Argilla on Hugging Face Spaces: If you want to run tutorials with external notebooks (e.g., Google Colab) and you have an account on Hugging Face, you can deploy Argilla on Spaces with a few clicks:</p> <p></p> <p>For details about configuring your deployment, check the official Hugging Face Hub guide.</p> <p>Launch Argilla using Argilla's quickstart Docker image: This is the recommended option if you want Argilla running on your local machine. Note that this option will only let you run the tutorial locally and not with an external notebook service.</p> <p>For more information on deployment options, please check the Deployment section of the documentation.</p>   Tip  This tutorial is a Jupyter Notebook. There are two options to run it:  - Use the Open in Colab button at the top of this page. This option allows you to run the notebook directly on Google Colab. Don't forget to change the runtime type to GPU for faster model training and inference. - Download the .ipynb file by clicking on the View source link at the top of the page. This option allows you to download the notebook and run it on your local machine or on a Jupyter notebook tool of your choice."},{"location":"_source/tutorials/add-suggestions-and-responses-005/#get-the-dataset","title":"Get the Dataset","text":"<p>As we uploaded the dataset that we created in the previous tutorial to both Argilla and HuggingFace Hub, we can now get the dataset. Let us see how we can pull the dataset from both.</p>"},{"location":"_source/tutorials/add-suggestions-and-responses-005/#from-argilla","title":"From Argilla","text":"<p>We can pull the dataset from Argilla by using the <code>from_argilla</code> method.</p>"},{"location":"_source/tutorials/add-suggestions-and-responses-005/#pull-the-original-dataset","title":"Pull the Original Dataset","text":""},{"location":"_source/tutorials/add-suggestions-and-responses-005/#add-responses","title":"Add <code>Responses</code>","text":"<p>Now that we have the original dataset and the dataset that we created in the previous tutorial, we can add the responses. The process is slightly different for the local FeedbackDataset and the RemoteFeedbackDataset. Let us see how we can do it for both.</p>"},{"location":"_source/tutorials/add-suggestions-and-responses-005/#add-suggestions","title":"Add <code>Suggestions</code>","text":"<p>As we have mentioned, suggestions are the predictions by any model of your preference to be added as suggested responses to our dataset. In this tutorial, we will be using the <code>cointegrated/rubert-tiny-bilingual-nli</code> model from the HuggingFace Hub to obtain our model predictions. To obtain the predictions, we will use the <code>pipeline</code> method from the <code>transformers</code> library, which makes it easy to use models for inference. Let us first load the model. To give us our zero-shot model, we also create a list of the labels that we want to predict.</p>"},{"location":"_source/tutorials/add-suggestions-and-responses-005/#publish-the-responses-and-suggestions-to-argilla","title":"Publish the Responses and Suggestions to Argilla","text":"<p>Now that we have added the responses and suggestions to our dataset, we can push the dataset to Argilla. We can do this by using the <code>publish</code> method.</p>"},{"location":"_source/tutorials/add-suggestions-and-responses-005/#to-huggingface-hub","title":"To HuggingFace Hub","text":"<p>If you would like to push the dataset you created to the HuggingFace Hub, you can simply use the <code>push_to_huggingface</code> method to upload it. Do not forget to create a model card as well, which will make the dataset more readable and understandable for the users.</p> <p>To be able to upload your dataset to the Hub, you must be logged in to the Hub. The following cell will log us with our previous token.</p> <p>If we don't have one already, we can obtain it from here (remember to set the write access).</p>"},{"location":"_source/tutorials/add-suggestions-and-responses-005/#conclusion","title":"Conclusion","text":"<p>In this tutorial, we have seen how we can add responses and suggestions to our dataset. Adding responses to your dataset given that you already have the annotated labels is a great way to save time and effort for your project. Similarly, adding suggestions to your dataset will make the annotation process much faster and easier for your annotators. Now, with the dataset we obtained, we can move on to training our model and computing the metrics. For more detailed info on how to utilize various tools, please refer to our practical guides</p>"},{"location":"_source/tutorials/assign-records-002/","title":"Assign records 002","text":"<p>First, let's install our dependencies and import the necessary libraries:</p> <pre><code>!pip install \"argilla-python\"\n!pip install datasets\n</code></pre> <pre><code>import argilla_sdk as rg\n</code></pre> <p>In order to run this notebook we will need some credentials to push and load datasets from <code>Argilla</code> and <code>\ud83e\udd17 Hub</code>, let's set them in the following cell:</p> <p>Log in to argilla:</p> <pre><code>client = rg.Argilla()\n</code></pre> <pre><code>dataset = client.datasets.get(name=\"end2end_textclassification\", workspace=\"argilla\")\n</code></pre> <p>If the dataset does not</p> <pre><code>dataset[0].fields\n</code></pre> <pre>\n<code>{'text': \"Wall St. Bears Claw Back Into the Black (Reuters) Reuters - Short-sellers, Wall Street's dwindling\\\\band of ultra-cynics, are seeing green again.\"}</code>\n</pre> <p>Feedback dataset supports multiple annotation for each record, allowing for straightforward aggregation and utilization. Simply ensure that your dataset is available in a shared workspace accessible to all team members. For detailed guidance on setting this up, refer to our comprehensive example on configuring users and workspaces.</p> <p>If you aim to specify a fixed number of annotations per record, it's advisable to implement a controlled overlap. To facilitate this, we offer a variety of options and workflows tailored to meet your needs.</p> <p>First, we will set our groups and the records. In this example, we'll be considering a scenario where we are collaborating with two teams, each consisting of five members and a thousand records that need to be annotated. This will be the first time we work with this teams, so we will only be providing their names.</p> <p>&gt; Note: The names of your members should be written in lower case.</p> <pre><code># Get all the users in the workspace\nworkspace_users = client.workspaces.get(name=\"argilla\").users\n\n# Split the users in to two groups\nusers_group_1 = workspace_users[: len(workspace_users) // 2]\nusers_group_2 = workspace_users[len(workspace_users) // 2 :]\n</code></pre> <p>1.1. Define groups from new users</p> <pre><code>tania = rg.User(username=\"tania\", password=\"taniascat\")\nbill = rg.User(username=\"jeroen\", password=\"jeroensdog\")\narmita = rg.User(username=\"armita\", password=\"amitasbird\")\ntim = rg.User(username=\"tim\", password=\"taniascat\")\njames = rg.User(username=\"james\", password=\"jamesfish\")\nrashida = rg.User(username=\"rashida\", password=\"rashidaslizard\")\n\nfor user in [tania, bill, armita, tim, james, rashida]:\n    client.users.create(user)\n</code></pre> <p>1.2 Define groups existing users</p> <pre><code>tania = client.users.get(username=\"tania\")\nbill = client.users.get(username=\"jeroen\")\narmita = client.users.get(username=\"armita\")\nrashida = client.users.get(username=\"rashida\")\ntim = rg.User(username=\"tim\", password=\"taniascat\")\njames = rg.User(username=\"james\", password=\"jamesfish\")\n\n# Add the users to the workspace\nclient.users.create(tim)\nclient.users.create(james)\n</code></pre> <p>We will use the <code>assign_records</code> function to assign the annotations. We will specify the following parameters:</p> <ul> <li><code>users</code>: This will be our dictionary with the groups.</li> <li><code>records</code>: The list of 1000 records.</li> <li><code>overlap</code>: We will set the overlap value to 1 (also known as zero overlap), indicating that the teams will annotate the samples independently, without any overlap. Since each group comprises five members, ultimately, each record will receive 5 annotations, as all members from the same team will be annotating the same records.</li> <li><code>shuffle</code>: To avoid any kind of bias, we will set it to <code>True</code>, so the samples are shuffled before the assignment.</li> </ul> <p>As we already mention, we have not yet worked with these annotators, so their users in Argilla will be automatically created.</p> <p>&gt; Note: The username and first name will be the names provided, the default password is \"12345678\" and they will have the role of \"annotator\".</p> <pre><code>users_group_1 = [tania, bill, armita]\nusers_group_2 = [tim, james, rashida]\n\nassignments = dataset.assign(groups=[users_group_1, users_group_2], overlap=1, shuffle=True)\nclient.datasets.update(dataset)\n</code></pre> <p></p>"},{"location":"_source/tutorials/assign-records-002/#assign-records-to-your-team","title":"Assign records to your team","text":"<p>This tutorial is part of a series in which we will get to know the <code>FeedbackDataset</code>. In this step, we will show the complete workflow to assign the records of our TextClassification dataset to a team. You can have a look at the previous tutorials for creating the dataset and configuring the users and workspaces. Feel free to check out the practical guides page for more in-depth information.</p>"},{"location":"_source/tutorials/assign-records-002/#table-of-contents","title":"Table of contents","text":"<ol> <li>Load a FeedbackDataset</li> <li>From Argilla</li> <li>From the HuggingFace Hub</li> <li>Full Overlap</li> <li>Controlled Overlap</li> <li>Working with Groups</li> <li>Working with a List of Users</li> <li>Conclusion</li> </ol>"},{"location":"_source/tutorials/assign-records-002/#running-argilla","title":"Running Argilla","text":"<p>For this tutorial, you will need to have an Argilla server running. There are two main options for deploying and running Argilla:</p> <p>Deploy Argilla on Hugging Face Spaces: If you want to run tutorials with external notebooks (e.g., Google Colab) and you have an account on Hugging Face, you can deploy Argilla on Spaces with a few clicks:</p> <p></p> <p>For details about configuring your deployment, check the official Hugging Face Hub guide.</p> <p>Launch Argilla using Argilla's quickstart Docker image: This is the recommended option if you want Argilla running on your local machine. Note that this option will only let you run the tutorial locally and not with an external notebook service.</p> <p>For more information on deployment options, please check the Deployment section of the documentation.</p>   Tip  This tutorial is a Jupyter Notebook. There are two options to run it:  - Use the Open in Colab button at the top of this page. This option allows you to run the notebook directly on Google Colab. Don't forget to change the runtime type to GPU for faster model training and inference. - Download the .ipynb file by clicking on the View source link at the top of the page. This option allows you to download the notebook and run it on your local machine or on a Jupyter notebook tool of your choice."},{"location":"_source/tutorials/assign-records-002/#pull-the-dataset","title":"Pull the Dataset","text":"<p>As we uploaded the dataset we created in the previous tutorial to both Argilla and HuggingFace Hub, we can pull the dataset from either of them. Let us see how we can pull the dataset from both.</p>"},{"location":"_source/tutorials/assign-records-002/#from-argilla","title":"From Argilla","text":"<p>We can pull the dataset from Argilla by using the <code>get</code> method.</p>"},{"location":"_source/tutorials/assign-records-002/#full-overlap","title":"Full Overlap","text":""},{"location":"_source/tutorials/assign-records-002/#controlled-overlap","title":"Controlled Overlap","text":""},{"location":"_source/tutorials/assign-records-002/#working-with-groups","title":"Working with Groups","text":"<p>This section will focus on describing all the steps to assign records to predefined groups of annotators.</p>"},{"location":"_source/tutorials/assign-records-002/#1-get-the-users-and-records","title":"1. Get the users and records","text":""},{"location":"_source/tutorials/assign-records-002/#2-assign-the-records","title":"2. Assign the records","text":""},{"location":"_source/tutorials/assign-records-002/#conclusion","title":"Conclusion","text":"<p>To sum up, we have shown how to assign records with full and controlled overlap. In scenarios involving controlled overlap, we explored the methods for distributing records among groups of annotators and a specified list of users through the use of <code>assign_records</code> and <code>assign_workspaces</code>. Additionally, we examined the technique for transferring these allocated records to their respective workspaces. In the next tutorial, we will see how we can add metadata properties to our dataset.</p>"},{"location":"_source/tutorials/configure-users-and-workspaces-000/","title":"Configure users and workspaces 000","text":"<p>First let's install our dependencies and import the necessary libraries:</p> <pre><code>!pip install \"argilla-python\"\n</code></pre> <pre><code>from argilla import Argilla\n</code></pre> <p>Log to argilla:</p> <pre><code>rg = Argilla(\n    api_url=\"http://localhost:6900/\",\n    api_key=\"admin.apikey\",\n)\n</code></pre> <pre><code>user = rg.User.get_me()\nuser\n</code></pre> <pre>\n<code>User(username='admin', first_name='Admin', role=&lt;Role.admin: 'admin'&gt;, last_name=None, password=None, id='5bb2a532-36f6-4af8-b5b8-ad2fbba4b7d3', api_key='admin.apikey', inserted_at='2024-01-31T18:48:52.994136', updated_at='2024-01-31T18:48:52.994136')</code>\n</pre> <p>As we can see, we are logged into the default <code>User</code> called <code>argilla</code>, which has the <code>owner</code> role. This <code>role</code> is allowed to create new <code>Users</code> and configure workspaces.</p> <pre><code>user = rg.User(\n        username=\"admin\",\n        first_name=\"Admin\",\n        role=\"admin\",\n    )\nuser.create()\n</code></pre> <pre><code>user.update(\n    first_name=\"Admin\",\n    last_name=\"AdminMcAdmin\",\n) \n</code></pre> <p>In this tutorial, we created some Argilla <code>Users</code> and <code>Workspaces</code>, and created some annotation team configurations.</p>"},{"location":"_source/tutorials/configure-users-and-workspaces-000/#configuring-users-and-workspaces","title":"Configuring <code>Users</code> and <code>Workspaces</code>","text":"<p>This tutorial is part of a series in which we will get to know Argilla's <code>Dataset</code>. In this step, we will show how to configure <code>Users</code> and <code>Workspaces</code>. If you need additional context, consult user management and workspace management.</p> <p></p>"},{"location":"_source/tutorials/configure-users-and-workspaces-000/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Configure Users</li> <li>Current active User</li> <li>Create User</li> <li>Update User</li> <li>Conclusion</li> </ol>"},{"location":"_source/tutorials/configure-users-and-workspaces-000/#running-argilla","title":"Running Argilla","text":"<p>For this tutorial, you will need to have an Argilla server running. There are two main options for deploying and running Argilla:</p> <p>Deploy Argilla on Hugging Face Spaces: If you want to run tutorials with external notebooks (e.g., Google Colab) and you have an account on Hugging Face, you can deploy Argilla on Spaces with a few clicks:</p> <p></p> <p>For details about configuring your deployment, check the official Hugging Face Hub guide.</p> <p>Launch Argilla using Argilla's quickstart Docker image: This is the recommended option if you want Argilla running on your local machine. Note that this option will only let you run the tutorial locally and not with an external notebook service.</p> <p>For more information on deployment options, please check the Deployment section of the documentation.</p>   Tip  This tutorial is a Jupyter Notebook. There are two options to run it:  - Use the Open in Colab button at the top of this page. This option allows you to run the notebook directly on Google Colab. Don't forget to change the runtime type to GPU for faster model training and inference. - Download the .ipynb file by clicking on the View source link at the top of the page. This option allows you to download the notebook and run it on your local machine or on a Jupyter notebook tool of your choice."},{"location":"_source/tutorials/configure-users-and-workspaces-000/#configure-users","title":"Configure <code>Users</code>","text":""},{"location":"_source/tutorials/configure-users-and-workspaces-000/#current-active-user","title":"Current active <code>User</code>","text":"<p>For this tutorial, we'll start with exploring the currently active user we've configured during the initialization. We can do this using our <code>Python</code> client or <code>CLI</code>.</p>"},{"location":"_source/tutorials/configure-users-and-workspaces-000/#create-user","title":"Create <code>User</code>","text":"<p>Next, we will create two new users with the role of <code>admin</code> and <code>annotator</code> to configure our first small team. For convenience and reproducibility, we will call them <code>admin</code> and <code>annotator</code>, and we'll set their <code>password</code> to the default <code>argilla.apikey</code>. Their <code>api_key</code> will be randomly generated according to safety standards. We will first create the <code>admin</code> user with our <code>Python</code> client.</p>"},{"location":"_source/tutorials/configure-users-and-workspaces-000/#update-user","title":"Update <code>User</code>","text":"<p>We can update a user using the update method and passing the new user informations. Existing arguments will remain the same if not updated.</p>"},{"location":"_source/tutorials/configure-users-and-workspaces-000/#conclusion","title":"Conclusion","text":""},{"location":"_source/tutorials/create-dataset-001/","title":"Create dataset 001","text":"<p>First let's install our dependencies and import the necessary libraries:</p> <pre><code>!pip install \"argilla-python\"\n!pip install datasets\n</code></pre> <pre><code>import argilla as rg\nfrom datasets import load_dataset\n</code></pre> <pre>\n<code>/home/ben/code/argilla-python/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n</code>\n</pre> <p>Connect to argilla:</p> <pre><code># papermill_description=logging-to-argilla\nclient = rg.Argilla()\n</code></pre> <p>For this tutorial we will use the ag_news dataset which can be downloaded from the \ud83e\udd17<code>hub</code>. We will load only the first 1000 items from the training sample.</p> <pre><code>ds = load_dataset(\"ag_news\", split=\"train[:1000]\")\nds\n</code></pre> <pre>\n<code>Downloading data: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 18.6M/18.6M [00:06&lt;00:00, 3.09MB/s]\nDownloading data: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1.23M/1.23M [00:00&lt;00:00, 2.32MB/s]\nGenerating train split: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 120000/120000 [00:00&lt;00:00, 1338233.95 examples/s]\nGenerating test split: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 7600/7600 [00:00&lt;00:00, 1354093.30 examples/s]\n</code>\n</pre> <pre>\n<code>Dataset({\n    features: ['text', 'label'],\n    num_rows: 1000\n})</code>\n</pre> <p>We will just load the first 1000 records for this tutorial, but feel free to test the full dataset.</p> <p>This dataset contains a collection of news articles (we can see the content in the <code>text</code> column), which have been asigned one of the following classification <code>labels</code>: World (0), Sports (1), Business (2), Sci/Tech (3).</p> <p>Let's use the task templates to create a feedback dataset ready for <code>text-classification</code>.</p> <pre><code>text_classification_dataset = rg.Dataset(\n    template=rg.Template.for_text_classification(\n        guidelines=\"Classify the articles into one of the four categories.\",\n        labels=[\"World\", \"Sports\", \"Business\", \"Sci/Tech\"],\n    )\n)\nclient.datasets.create(text_classification_dataset)\n</code></pre> <p>We could compare this dataset with the custom configuration we would use previously (we can take a look at the custom configuration for more information on the creation of a <code>Dataset</code> when we want a finer control):</p> <pre><code>custom_text_classification_template = rg.Template(\n    guidelines=\"Classify the articles into one of the four categories.\",\n    fields=[\n        rg.Field(name=\"text\", title=\"Text from the article\"),\n        rg.Field(name=\"context\", title=\"Context from the article\"),\n    ],\n    questions=[\n        rg.Question(\n            name=\"label\",\n            title=\"In which category does this article fit?\",\n            required=True,\n            settings=rg.QuestionSettings.MultiLabel(\n                options=rg.LabelOption.from_labels([\"World\", \"Sports\", \"Business\", \"Sci/Tech\"])\n            ),\n        )\n    ],\n)\ncustom_text_classification_dataset = rg.Dataset(template=custom_text_classification_template)\nclient.datasets.create(custom_text_classification_dataset)\n</code></pre> <pre>\n<code>FeedbackDataset(\n   fields=[TextField(name='text', title='Text from the article', required=True, type='text', use_markdown=False)]\n   questions=[LabelQuestion(name='label', title='In which category does this article fit?', description=None, required=True, type='label_selection', labels={'World': '0', 'Sports': '1', 'Business': '2', 'Sci/Tech': '3'}, visible_labels=None)]\n   guidelines=Classify the articles into one of the four categories.)\n   metadata_properties=[])\n)</code>\n</pre> <p>The next step once we have our <code>Dataset</code> created is adding the FeedbackRecords to it.</p> <p>In order to create our records we can just loop over the items in the <code>datasets.Dataset</code>.</p> <pre><code>for i, item in enumerate(ds):\n    text_classification_dataset.records.add(\n        rg.Record(\n            fields={\n                \"text\": item[\"text\"],\n            },\n            external_id=f\"record-{i}\",\n        )\n    )\n</code></pre> <p>If we had our data in a different format, let's say a <code>csv</code> file, maybe it's more direct to read the data using pandas for that.</p> <p>We will transform our dataset to pandas format for this example, and the remaining <code>FeedbackRecord</code> creation remains just the same:</p> <pre><code>df_dataset = ds.to_pandas()\ndf_dataset.head()\n</code></pre> text label 0 Wall St. Bears Claw Back Into the Black (Reute... 2 1 Carlyle Looks Toward Commercial Aerospace (Reu... 2 2 Oil and Economy Cloud Stocks' Outlook (Reuters... 2 3 Iraq Halts Oil Exports from Main Southern Pipe... 2 4 Oil prices soar to all-time record, posing new... 2 <p>Let's add our records to the dataset:</p> <pre><code>for i, item in df_dataset.iterrows():\n    text_classification_dataset.records.add(\n        rg.Record(\n            fields={\n                \"text\": item[\"text\"],\n            },\n            external_id=f\"record-{i}\",\n        )\n    )\n</code></pre> <pre><code>client.datasets.publish(text_classification_dataset)\n</code></pre> <p>By now we have our dataset with the texts ready to be labeled, let's push it to <code>Argilla</code>.</p> <p>If we go to our <code>Argilla</code> instance we should see a similar screen like the following.</p> <p></p> <pre><code>dataset = client.datasets.get(text_classification_dataset.id)\nlocal_dataset = dataset.pull()\n# We can now convert to our favorite format\n\ndf = dataset.to_pandas()\nds = dataset.to_datasets()\n</code></pre> <p>In this tutorial we created an <code>Argilla</code> <code>FeedbackDataset</code> for text classification, starting from ag_news.</p> <p>We created a <code>FeedbackDataset</code> for text classification with a <code>LabelQuestion</code>, from data stored as a <code>datasets.Dataset</code> and a <code>pandas.DataFrame</code>. This dataset was pushed both to <code>Argilla</code> where we can curate and label the records, and finally pushed it to the \ud83e\udd17<code>hub</code>.</p> <p>To learn more about how to work with the <code>FeedbackDataset</code> check the cheatsheet. To continue with assigning records to annotators, you can refer to the next tutorial.</p>"},{"location":"_source/tutorials/create-dataset-001/#creating-a-dataset","title":"Creating a <code>Dataset</code>","text":"<p>This tutorial is part of a series in which we will get to know the <code>Dataset</code>. Before starting this tutorial, you need to do the tutorial on configuring users and workspaces. In this step, we will show how to configure a <code>Dataset</code> and add <code>Records</code> to it.</p> <p>We will start by creating a basic dataset using the ag_news dataset as an example and push it to <code>Argilla</code> and the Hugging Face <code>hub</code>.</p>"},{"location":"_source/tutorials/create-dataset-001/#running-argilla","title":"Running Argilla","text":"<p>For this tutorial, you will need to have an Argilla server running. There are two main options for deploying and running Argilla:</p> <p>Deploy Argilla on Hugging Face Spaces: If you want to run tutorials with external notebooks (e.g., Google Colab) and you have an account on Hugging Face, you can deploy Argilla on Spaces with a few clicks:</p> <p></p> <p>For details about configuring your deployment, check the official Hugging Face Hub guide.</p> <p>Launch Argilla using Argilla's quickstart Docker image: This is the recommended option if you want Argilla running on your local machine. Note that this option will only let you run the tutorial locally and not with an external notebook service.</p> <p>For more information on deployment options, please check the Deployment section of the documentation.</p>   Tip  This tutorial is a Jupyter Notebook. There are two options to run it:  - Use the Open in Colab button at the top of this page. This option allows you to run the notebook directly on Google Colab. Don't forget to change the runtime type to GPU for faster model training and inference. - Download the .ipynb file by clicking on the View source link at the top of the page. This option allows you to download the notebook and run it on your local machine or on a Jupyter notebook tool of your choice."},{"location":"_source/tutorials/create-dataset-001/#configure-a-dataset","title":"Configure a <code>Dataset</code>","text":""},{"location":"_source/tutorials/create-dataset-001/#add-records","title":"Add <code>Records</code>","text":""},{"location":"_source/tutorials/create-dataset-001/#from-a-hugging-face-dataset","title":"From a Hugging Face <code>dataset</code>","text":""},{"location":"_source/tutorials/create-dataset-001/#from-a-pandasdataframe","title":"From a <code>pandas.DataFrame</code>","text":""},{"location":"_source/tutorials/create-dataset-001/#publish-our-dataset-to-argilla-for-feedback","title":"Publish our dataset to Argilla for Feedback","text":""},{"location":"_source/tutorials/create-dataset-001/#download-our-dataset-for-use","title":"Download our dataset for use","text":""},{"location":"_source/tutorials/create-dataset-001/#conclusion","title":"Conclusion","text":""},{"location":"_source/tutorials/end-to-end-argilla-tutorial/","title":"End to end argilla tutorial","text":"<pre><code>import argilla_sdk as rg\n</code></pre> <pre><code># Connecting to an Argilla server\n# Creating a workspace\n# Creating users\n\nclient = rg.Client(api_rul=\"http://localhost:6900\", api_key=\"admin.apikey\")\n# As now, the client creates a default workspace, user and role\n</code></pre> <pre><code># Connecting to an Argilla server\nclient = rg.Client(api_rul=\"http://localhost:6900\", api_key=\"admin.apikey\")\n\n# Creating a workspace\nworkspace = rg.Workspace(name=\"MyWorkspace\")\nclient.workspace.create(workspace)\n\n# Creating users\nuser = rg.User(name=\"John Doe\", role=\"admin\", workspace=workspace.name)\nclient.user.create(user)\n</code></pre> <pre><code># Connecting to an Argilla server\nlocal_client = rg.Client(api_rul=\"http://localhost:6900\", api_key=\"admin.apikey\")\nproduction_client = rg.Client(api_rul=\"http://argilla.production.net\", api_key=\"admin.apikey\")\n\n# Creating a workspace\nworkspace = rg.Workspace(name=\"MyWorkspace\")\nlocal_client.workspace.create(workspace)\ud83d\ude80\nproduction_client.workspace.create(workspace)\n\n# Creating users\nuser = rg.User(name=\"John Doe\", role=\"admin\", workspace=workspace.name)\nclient.user.create(user)\n\n# We can take a user from a workspace and create it in another workspace\nuser_from_local = client.user.get(name=\"John Doe\", workspace=local_client.workspace.name)\nproduction_client.user.create(user_from_local)\n</code></pre> <pre><code>import pandas as pd\n\n\ndataset_list = [{\"text\": \"Hello, world!\"}, {\"text\": \"I'm a sentence.\"}]\n\ndataset_dict = {\"text\": [\"Hello, world!\", \"I'm a sentence.\"]}\n\ndataset_df = pd.DataFrame(dataset_dict)\n</code></pre> <pre><code># Defining the dataset's feedback task\n# Adding records to the dataset\n\ndataset = rg.Dataset.from_list(dataset_list, name=\"MyListDataset\", field_keys=[\"text\"])\nclient.dataset.create(dataset)\n# we raise an error if the dicts contain different keys\n\ndataset = rg.Dataset.from_dict(dataset_dict, name=\"MyDictDataset\", field_keys=[\"text\"])\nclient.dataset.create(dataset)\n# we raise an error if the lists are different lengths\n\ndataset = rg.Dataset.from_df(dataset_df, name=\"MyDfDataset\", field_keys=[\"text\"])\nclient.dataset.create(dataset)\n</code></pre> <pre><code># Defining the dataset's feedback task\n\ntemplate = rg.Template(\n    guidelines=\"Classify the articles into one of the four categories.\",\n    fields=[\n        rg.Field(name=\"text\", title=\"Text from the article\"),\n    ],\n    questions=[\n        rg.Question(\n            name=\"label\",\n            title=\"In which category does this article fit?\",\n            required=True,\n            settings=rg.QuestionSettings.MultiLabel(options=rg.LabelOption.from_labels([\"greeting\", \"statement\"])),\n        )\n    ],\n)\n\n# Adding records to the dataset\ndataset = rg.Dataset.from_list(dataset_list, name=\"MyListDataset\", template=template)\nclient.dataset.create(dataset)\n</code></pre> <pre><code># Defining the dataset's feedback task\n\ntemplate = rg.Template(\n    guidelines=\"Classify the articles into one of the four categories.\",\n    fields=[\n        rg.Field(name=\"text\", title=\"Text from the article\"),\n    ],\n    questions=[\n        rg.Question(\n            name=\"label\",\n            title=\"In which category does this article fit?\",\n            required=True,\n            settings=rg.QuestionSettings.MultiLabel(options=rg.LabelOption.from_labels([\"greeting\", \"statement\"])),\n        )\n    ],\n)\n\ndataset = rg.Dataset(name=\"MyListDataset\", template=template)\n\n# Adding records to the dataset\nfor i, item in enumerate(ds):\n    dataset.records.add(\n        rg.Record(\n            fields={\n                \"text\": item[\"text\"],\n            },\n            external_id=f\"record-{i}\",\n        )\n    )\n\nclient.dataset.create(dataset)\n</code></pre> <pre><code>suggestion_list = [{\"text\": \"Hello, world!\", \"label\": \"greeting\"}, {\"text\": \"I'm a sentence.\", \"label\": \"statement\"}]\nresponse_list = [{\"text\": \"Hello, world!\", \"label\": \"greeting\"}, {\"text\": \"I'm a sentence.\", \"label\": \"statement\"}]\n</code></pre> <pre><code># Adding suggestions and responses\ndataset = dataset.suggestions.from_list(dataset_list)\ndataset = dataset.responses.from_list(dataset_list)\nclient.datasetx.update(dataset)\n# we raise an error if the dicts contain different key\n</code></pre> <pre><code># Assigning records to user/users\ndataset = dataset.assign(groups=\"even_split\", overlap=1, shuffle=True)\n</code></pre> <pre><code># Assigning records to user/users\njames = rg.User(username=\"james\", password=\"jamesfish\")\nrashida = rg.User(username=\"rashida\", password=\"rashidaslizard\")\ndataset = dataset.assign(splits=[james, rashida], overlap=1, shuffle=True)\nclient.dataset.update(dataset)\n</code></pre> <pre><code># Adding suggestions\nfor record in dataset.records:\n    record.suggestions.add(\n        rg.Suggestion(\n            user=user,\n            fields={\n                \"label\": \"greeting\",\n            },\n        )\n    )\n\n# Adding responses\nfor record in dataset.records:\n    record.responses.add(\n        rg.Response(\n            user=user,\n            fields={\n                \"label\": \"greeting\",\n            },\n        )\n    )\n\nclient.datasets.update(dataset)\n</code></pre> <pre><code># Assigning records to user/users\ntania = rg.User(username=\"tania\", password=\"taniascat\")\nbill = rg.User(username=\"jeroen\", password=\"jeroensdog\")\narmita = rg.User(username=\"armita\", password=\"amitasbird\")\ntim = rg.User(username=\"tim\", password=\"taniascat\")\njames = rg.User(username=\"james\", password=\"jamesfish\")\nrashida = rg.User(username=\"rashida\", password=\"rashidaslizard\")\n\nfor user in [tania, bill, armita, tim, james, rashida]:\n    client.users.create(user)\n\nusers_group_1 = [tania, bill, armita]\nusers_group_2 = [tim, james, rashida]\n\ndataset = dataset.assign(splits=[users_group_1, users_group_2], overlap=1, shuffle=True)\nclient.datasets.update(dataset)\n</code></pre> <pre><code># Adding suggestions\nfor record in dataset.records:\n    record.suggestions.add(\n        rg.Suggestion(\n            user=user,\n            fields={\n                \"label\": \"greeting\",\n            },\n        )\n    )\n\n# Adding responses\nfor record in dataset.records:\n    record.responses.add(\n        rg.Response(\n            user=user,\n            fields={\n                \"label\": \"greeting\",\n            },\n        )\n    )\n\nclient.datasets.update(dataset)\n</code></pre>"},{"location":"_source/tutorials/end-to-end-argilla-tutorial/#end-to-end-tutorial-for-using-the-ne-argilla-sdk","title":"End to end tutorial for using the ne Argilla SDK","text":"<p>This tutorial encomposses the full workflow of using the Argilla SDK to manage human feedback tasks. The main three areas are:</p> <ol> <li>Setting up a project:</li> <li>Connecting to an Argilla server:</li> <li>Creating a workspace:</li> <li>Creating users:</li> <li>Creating a dataset:</li> <li>Defining the dataset's feedback task:</li> <li>Adding records to the dataset:</li> <li>Labelling the dataset:</li> <li>Assigning records to users:</li> <li>Adding suggestions and responses:</li> </ol> <p>We will work through each of these areas and show three depths of control: A line, B line, and C line. The A line is the simplest and most abstracted, the B line is a bit more detailed, and the C line is the most detailed and flexible.</p>"},{"location":"_source/tutorials/end-to-end-argilla-tutorial/#1-setting-up-a-project","title":"1. Setting up a project","text":"<ul> <li>Connecting to an Argilla server</li> <li>Creating a workspace</li> <li>Creating users</li> </ul>"},{"location":"_source/tutorials/end-to-end-argilla-tutorial/#a-line-setting-up-a-project","title":"A line : Setting up a project \ud83e\udeab","text":""},{"location":"_source/tutorials/end-to-end-argilla-tutorial/#b-line-setting-up-a-project","title":"B Line: Setting up a project \ud83d\udd0b","text":""},{"location":"_source/tutorials/end-to-end-argilla-tutorial/#c-line-setting-up-a-project","title":"C line: Setting up a project \ud83d\ude80","text":""},{"location":"_source/tutorials/end-to-end-argilla-tutorial/#2-creating-a-dataset","title":"2. Creating a dataset","text":"<ul> <li>Defining the dataset's feedback task</li> <li>Adding records to the dataset</li> </ul>"},{"location":"_source/tutorials/end-to-end-argilla-tutorial/#a-line-creating-a-dataset","title":"A line : Creating a dataset \ud83e\udeab","text":""},{"location":"_source/tutorials/end-to-end-argilla-tutorial/#b-line-creating-a-dataset","title":"B Line: Creating a dataset \ud83d\udd0b","text":""},{"location":"_source/tutorials/end-to-end-argilla-tutorial/#c-line-creating-a-dataset","title":"C line: Creating a dataset \ud83d\ude80","text":""},{"location":"_source/tutorials/end-to-end-argilla-tutorial/#3-labelling-the-dataset","title":"3. Labelling the dataset","text":"<ul> <li>Assigning records to users</li> <li>Adding suggestions and responses</li> </ul>"},{"location":"_source/tutorials/end-to-end-argilla-tutorial/#a-line-labelling-the-dataset","title":"A line: Labelling the dataset \ud83e\udeab","text":""},{"location":"_source/tutorials/end-to-end-argilla-tutorial/#b-line-labelling-the-dataset","title":"B Line: Labelling the dataset \ud83d\udd0b","text":""},{"location":"_source/tutorials/end-to-end-argilla-tutorial/#c-line-labelling-the-dataset","title":"C line: Labelling the dataset \ud83d\ude80","text":""},{"location":"how_to_guides/","title":"How to guides","text":"<p>These are the how-to guides for the Argilla-python SDK. They provide step-by-step instructions for common scenarios, including detailed explanations and code samples.</p> <ul> <li> <p> Manage your annotation team</p> <p>Learn how to Create, Read, Update and Delete (CRUD) a <code>Workspace</code> or <code>User</code> in Argilla.</p> <p> Getting started</p> </li> <li> <p> Define and change your dataset settings</p> <p>Learn how to Create, Read, Update and Delete (CRUD) the <code>Settings</code> for a <code>Dataset</code>, which are made up of settings for <code>Fields</code>,<code>Questions</code>,  <code>Metadata</code> and <code>Vectors</code>.</p> <p> Reference</p> </li> <li> <p> Work with records in your dataset</p> <p>Learn how to Create, Read, Update and Delete (CRUD) the values for a<code>Record</code>, which are made up of <code>Suggestions</code>,<code>Responses</code>,  <code>Metadata</code> and <code>Vectors</code>.</p> <p> Customization</p> </li> <li> <p> Query, filter and export data</p> <p>Learn how to apply export <code>Records</code> with the right filters and queries.</p> <p> License</p> </li> <li> <p> Distribute annotation work</p> <p>Learn how to distribute <code>Records</code> to <code>Users</code> and <code>Workspaces</code> for managing annotation efforts the way you require.</p> <p> License</p> </li> <li> <p> Evaluate annotators and models with metrics and KPIs</p> <p>Learn how to evaluate <code>Users</code> and Models with <code>Metrics</code> and <code>KPIs</code> to ensure the quality of your data, models and annotations.</p> <p> License</p> </li> <li> <p> Fine-tune a model</p> <p>Learn how to fine-tune your own model with <code>transformers</code> and take ownership of your data and models.</p> <p> License</p> </li> </ul>"},{"location":"how_to_guides/fine_tune/","title":"Fine-tune","text":"<p>These are the how-to guides for the Argilla-python SDK. They provide step-by-step instructions for common scenarios, including detailed explanations and code samples.</p>"},{"location":"how_to_guides/dataset/","title":"Dataset","text":"<p>test</p>"},{"location":"how_to_guides/dataset/fields/","title":"Fields","text":""},{"location":"how_to_guides/dataset/metadata/","title":"Metadata","text":""},{"location":"how_to_guides/dataset/questions/","title":"Questions","text":""},{"location":"how_to_guides/dataset/vectors/","title":"Vectors","text":""},{"location":"how_to_guides/record/metadata/","title":"Metadata","text":""},{"location":"how_to_guides/record/vectors/","title":"Vectors","text":""},{"location":"how_to_guides/team/user/","title":"Users","text":"<p>A user in Argilla is an authorized person who can access the UI and use the Python client and CLI in a running Argilla instance.</p>"},{"location":"how_to_guides/team/workspace/","title":"Workspaces","text":"<p>Placeholder text</p>"},{"location":"integrations/","title":"argilla-sdk","text":"<ul> <li> <p> Distilabel</p> <p><code>distilabel</code> is a lightweight, easy-to-use, and scalable framework for AI feedback and synthetic data generation which directly integrates with Argilla. It is designed to help you quickly and easily create high-quality labeled data for your LLMs.</p> <p> Tutorial</p> </li> <li> <p> Argilla Metrics</p> <p><code>argilla-metrics</code> is a self-maintained library for tracking and evaluating the performance of your models and annotators.</p> <p> Tutorial</p> </li> <li> <p> Llama-index</p> <p><code>argilla-llama-index</code> is LLM orchestration framework. The integration with Argilla allows you to easily monitor pipeline traces and adding human feedback to your them.</p> <p> Tutorial</p> </li> <li> <p> Haystack</p> <p><code>argilla-haystack</code> is LLM orchestration framework. The integration with Argilla allows you to easily monitor pipeline traces and adding human feedback to your them.</p> <p> Tutorial</p> </li> <li> <p> Unstructured</p> <p><code>argilla-unstructured</code> is an ETL framework for LLM data. The integration with Argilla allows you to easily store and evaluate data from ETL pipelines and improve the quality using a human touch.</p> <p> Tutorial</p> </li> </ul>"},{"location":"overview/changelog/","title":"Changelog","text":"<p>Placeholder tex</p>"},{"location":"overview/community/","title":"Community","text":""},{"location":"overview/community/#community","title":"Community","text":"<p>We are an open-source community-driven project and we love to hear from you. Here are some ways to get involved:</p> <ul> <li> <p>Community Meetup: listen in or present during one of our bi-weekly events.</p> </li> <li> <p>Slack: get direct support from the community.</p> </li> <li> <p>Roadmap: plans change but we love to discuss those with our community so feel encouraged to participate.</p> </li> </ul>"},{"location":"overview/faq/","title":"Quickstart","text":"<p>Placeholder text</p>"},{"location":"overview/quickstart/","title":"Quickstart","text":"<p>Placeholder text</p>"},{"location":"overview/community/","title":"Community","text":""},{"location":"overview/community/#community","title":"Community","text":"<p>We are an open-source community-driven project and we love to hear from you. Here are some ways to get involved:</p> <ul> <li> <p>Community Meetup: listen in or present during one of our bi-weekly events.</p> </li> <li> <p>Slack: get direct support from the community.</p> </li> <li> <p>Roadmap: plans change but we love to discuss those with our community so feel encouraged to participate.</p> </li> </ul>"},{"location":"overview/community/contributor/","title":"GitHub workflows","text":""},{"location":"overview/community/contributor/#community","title":"Community","text":"<p>We are an open-source community-driven project and we love to hear from you. Here are some ways to get involved:</p> <ul> <li> <p>Community Meetup: listen in or present during one of our bi-weekly events.</p> </li> <li> <p>Slack: get direct support from the community.</p> </li> <li> <p>Roadmap: plans change but we love to discuss those with our community so feel encouraged to participate.</p> </li> </ul>"},{"location":"overview/community/developer/","title":"Development environment","text":""},{"location":"overview/community/developer/#community","title":"Community","text":"<p>We are an open-source community-driven project and we love to hear from you. Here are some ways to get involved:</p> <ul> <li> <p>Community Meetup: listen in or present during one of our bi-weekly events.</p> </li> <li> <p>Slack: get direct support from the community.</p> </li> <li> <p>Roadmap: plans change but we love to discuss those with our community so feel encouraged to participate.</p> </li> </ul>"},{"location":"overview/concepts/","title":"Argilla concepts","text":"<p>Placeholder text</p>"},{"location":"overview/installation/","title":"argilla-sdk","text":"<p>Placeholder text</p>"},{"location":"overview/installation/telemetry/","title":"argilla-sdk","text":"<p>Placeholder text</p>"},{"location":"reference/SUMMARY/","title":"SUMMARY","text":"<ul> <li>argilla_sdk<ul> <li>client</li> <li>responses</li> <li>suggestions</li> </ul> </li> </ul>"},{"location":"reference/argilla_sdk/client/","title":"client","text":""},{"location":"reference/argilla_sdk/client/#argilla_sdk.client.Datasets","title":"<code>Datasets</code>","text":"<p>             Bases: <code>Sequence</code></p> <p>A collection of datasets. It can be used to create a new dataset or to get an existing one.</p> Source code in <code>src/argilla_sdk/client.py</code> <pre><code>class Datasets(Sequence):\n    \"\"\"A collection of datasets. It can be used to create a new dataset or to get an existing one.\"\"\"\n\n    def __init__(self, client: \"Argilla\") -&gt; None:\n        self._client = client\n        self._api = client.api.datasets\n\n    def __call__(self, name: str, workspace: \"Workspace\", **kwargs) -&gt; \"Dataset\":\n        from argilla_sdk.datasets._resource import Dataset\n\n        workspace_id = workspace.id\n        dataset_models = self._api.list(workspace_id=workspace_id)\n\n        for model in dataset_models:\n            if model.name == name:\n                return Dataset(_model=model, client=self._client)\n\n        return Dataset(name=name, workspace_id=workspace_id, client=self._client, **kwargs)\n\n    @overload\n    @abstractmethod\n    def __getitem__(self, index: int) -&gt; \"Dataset\": ...\n\n    @overload\n    @abstractmethod\n    def __getitem__(self, index: slice) -&gt; Sequence[\"Dataset\"]: ...\n\n    def __getitem__(self, index) -&gt; \"Dataset\":\n        from argilla_sdk.datasets._resource import Dataset\n\n        model = self._api.list()[index]\n        return Dataset(client=self._client, _model=model)\n\n    def __len__(self) -&gt; int:\n        return len(self._api.list())\n</code></pre>"},{"location":"reference/argilla_sdk/client/#argilla_sdk.client.Users","title":"<code>Users</code>","text":"<p>             Bases: <code>Sequence</code></p> <p>A collection of users. It can be used to create a new user or to get an existing one.</p> Source code in <code>src/argilla_sdk/client.py</code> <pre><code>class Users(Sequence):\n    \"\"\"A collection of users. It can be used to create a new user or to get an existing one.\"\"\"\n\n    def __init__(self, client: \"Argilla\") -&gt; None:\n        self._client = client\n        self._api = client.api.users\n\n    def __call__(self, username: str, **kwargs) -&gt; \"User\":\n        from argilla_sdk.users._resource import User\n\n        user_models = self._api.list()\n        for model in user_models:\n            if model.username == username:\n                return User(_model=model, client=self._client)\n\n        return User(username=username, client=self._client, **kwargs)\n\n    @overload\n    @abstractmethod\n    def __getitem__(self, index: int) -&gt; \"User\": ...\n\n    @overload\n    @abstractmethod\n    def __getitem__(self, index: slice) -&gt; Sequence[\"User\"]: ...\n\n    def __getitem__(self, index):\n        from argilla_sdk.users._resource import User\n\n        model = self._api.list()[index]\n        return User(client=self._client, _model=model)\n\n    def __len__(self) -&gt; int:\n        return len(self._api.list())\n</code></pre>"},{"location":"reference/argilla_sdk/client/#argilla_sdk.client.Workspaces","title":"<code>Workspaces</code>","text":"<p>             Bases: <code>Sequence</code></p> <p>A collection of workspaces. It can be used to create a new workspace or to get an existing one.</p> Source code in <code>src/argilla_sdk/client.py</code> <pre><code>class Workspaces(Sequence):\n    \"\"\"A collection of workspaces. It can be used to create a new workspace or to get an existing one.\"\"\"\n\n    def __init__(self, client: \"Argilla\") -&gt; None:\n        self._client = client\n        self._api = client.api.workspaces\n\n    def __call__(self, name: str, **kwargs) -&gt; \"Workspace\":\n        from argilla_sdk.workspaces._resource import Workspace\n\n        workspace_models = self._api.list()\n\n        for model in workspace_models:\n            if model.name == name:\n                return Workspace(_model=model, client=self._client)\n\n        return Workspace(name=name, client=self._client, **kwargs)\n\n    @overload\n    @abstractmethod\n    def __getitem__(self, index: int) -&gt; \"Workspace\": ...\n\n    @overload\n    @abstractmethod\n    def __getitem__(self, index: slice) -&gt; Sequence[\"Workspace\"]: ...\n\n    def __getitem__(self, index: int) -&gt; \"Workspace\":\n        from argilla_sdk.workspaces._resource import Workspace\n\n        model = self._api.list()[index]\n        return Workspace(client=self._client, _model=model)\n\n    def __len__(self) -&gt; int:\n        return len(self._api.list())\n</code></pre>"},{"location":"reference/argilla_sdk/responses/","title":"responses","text":""},{"location":"reference/argilla_sdk/responses/#argilla_sdk.responses.Response","title":"<code>Response</code>","text":"<p>             Bases: <code>Resource</code></p> <p>Class for interacting with Argilla Responses of records</p> Source code in <code>src/argilla_sdk/responses.py</code> <pre><code>class Response(Resource):\n    \"\"\"Class for interacting with Argilla Responses of records\"\"\"\n\n    _model: ResponseModel\n\n    def __init__(\n        self,\n        question_name: str,\n        value: str,\n        user_id: str,\n        status: ResponseStatus = \"draft\",\n    ) -&gt; None:\n        \"\"\"Initializes a Response with a user_id and value\"\"\"\n        self._model = ResponseModel(\n            values=self.__create_response_values(question_name, value),\n            status=status,\n            user_id=user_id,\n        )\n\n    ####################\n    # Public Interface #\n    ####################\n\n    @property\n    def question_name(self) -&gt; str:\n        \"\"\"Returns the question name of the Response\"\"\"\n        return list(self._model.values.keys())[0]\n\n    @property\n    def value(self) -&gt; str:\n        \"\"\"Returns the value of the Response\"\"\"\n        return self._model.values[self.question_name][\"value\"]\n\n    @property\n    def user_id(self) -&gt; str:\n        \"\"\"Returns the user_id of the Response\"\"\"\n        return self._model.user_id\n\n    @property\n    def status(self) -&gt; ResponseStatus:\n        \"\"\"Returns the status of the Response\"\"\"\n        return self._model.status\n\n    @classmethod\n    def from_model(cls, model: ResponseModel) -&gt; \"Response\":\n        \"\"\"Creates a Response from a ResponseModel\"\"\"\n        question_name = list(model.values.keys())[0]\n        value = model.values[question_name][\"value\"]\n        user_id = str(model.user_id)\n        status = model.status\n        return cls(question_name, value, user_id, status)\n\n    def serialize(self) -&gt; dict[str, Any]:\n        \"\"\"Serializes the Response to a dictionary\"\"\"\n        model_dict = self._model.model_dump()\n        model_dict[\"question_name\"] = self.question_name\n        model_dict[\"value\"] = self.value\n        return model_dict\n\n    #####################\n    # Private Interface #\n    #####################\n\n    def __create_response_values(self, question_name, value):\n        return {question_name: {\"value\": value}}\n\n    @classmethod\n    def from_model(cls, model: ResponseModel) -&gt; \"Response\":\n        \"\"\"Creates a Response from a ResponseModel\"\"\"\n        question_name = list(model.values.keys())[0]\n        value = model.values[question_name][\"value\"]\n        user_id = str(model.user_id)\n        status = model.status\n        return cls(question_name, value, user_id, status)\n</code></pre>"},{"location":"reference/argilla_sdk/responses/#argilla_sdk.responses.Response.question_name","title":"<code>question_name: str</code>  <code>property</code>","text":"<p>Returns the question name of the Response</p>"},{"location":"reference/argilla_sdk/responses/#argilla_sdk.responses.Response.status","title":"<code>status: ResponseStatus</code>  <code>property</code>","text":"<p>Returns the status of the Response</p>"},{"location":"reference/argilla_sdk/responses/#argilla_sdk.responses.Response.user_id","title":"<code>user_id: str</code>  <code>property</code>","text":"<p>Returns the user_id of the Response</p>"},{"location":"reference/argilla_sdk/responses/#argilla_sdk.responses.Response.value","title":"<code>value: str</code>  <code>property</code>","text":"<p>Returns the value of the Response</p>"},{"location":"reference/argilla_sdk/responses/#argilla_sdk.responses.Response.__init__","title":"<code>__init__(question_name, value, user_id, status='draft')</code>","text":"<p>Initializes a Response with a user_id and value</p> Source code in <code>src/argilla_sdk/responses.py</code> <pre><code>def __init__(\n    self,\n    question_name: str,\n    value: str,\n    user_id: str,\n    status: ResponseStatus = \"draft\",\n) -&gt; None:\n    \"\"\"Initializes a Response with a user_id and value\"\"\"\n    self._model = ResponseModel(\n        values=self.__create_response_values(question_name, value),\n        status=status,\n        user_id=user_id,\n    )\n</code></pre>"},{"location":"reference/argilla_sdk/responses/#argilla_sdk.responses.Response.from_model","title":"<code>from_model(model)</code>  <code>classmethod</code>","text":"<p>Creates a Response from a ResponseModel</p> Source code in <code>src/argilla_sdk/responses.py</code> <pre><code>@classmethod\ndef from_model(cls, model: ResponseModel) -&gt; \"Response\":\n    \"\"\"Creates a Response from a ResponseModel\"\"\"\n    question_name = list(model.values.keys())[0]\n    value = model.values[question_name][\"value\"]\n    user_id = str(model.user_id)\n    status = model.status\n    return cls(question_name, value, user_id, status)\n</code></pre>"},{"location":"reference/argilla_sdk/responses/#argilla_sdk.responses.Response.serialize","title":"<code>serialize()</code>","text":"<p>Serializes the Response to a dictionary</p> Source code in <code>src/argilla_sdk/responses.py</code> <pre><code>def serialize(self) -&gt; dict[str, Any]:\n    \"\"\"Serializes the Response to a dictionary\"\"\"\n    model_dict = self._model.model_dump()\n    model_dict[\"question_name\"] = self.question_name\n    model_dict[\"value\"] = self.value\n    return model_dict\n</code></pre>"},{"location":"reference/argilla_sdk/suggestions/","title":"suggestions","text":""},{"location":"tutorials/","title":"Tutorials","text":"<p>These are the tutorials for the Argilla-python SDK. They provide step-by-step instructions for common scenarios, including detailed explanations and code samples.</p> <ul> <li> <p> Few-shot text classification with SetFit</p> <p>In this tutorial, you will learn how to use bulk labeling with semantic search from <code>sentence-transformers</code> embeddings and how to fine-tune a  <code>SetFit</code> model for classification.</p> <p> Tutorial</p> </li> <li> <p> Zero-shot token classification with GliNER</p> <p>In this tutorial, you will learn how to use zero-shot predictions from <code>GliNER</code> to fine-tune a span classification model with <code>SpanMarker</code>.</p> <p> Tutorial</p> </li> <li> <p> Bootstrap projects using LLMs</p> <p>In this tutorial, you will learn how to use LLMs to bootstrap projects for tasks like text classification, token classification, relationship extraction and summarization using <code>spacy-llm</code></p> <p> Tutorial</p> </li> <li> <p> Multi-modal projects</p> <p>In this tutorial, you will learn how to use Argilla for basic multi-modal projects with PDFs, images, video and audio. We will bulk label using <code>sentence-transformers</code> CLIP embeddings and fine-tune a model with <code>transformers</code>.</p> <p> Tutorial</p> </li> <li> <p> Monitor for data and model drift</p> <p>In this tutorial, you will learn you can keep track of model performance and data quality using <code>BerTopic</code> and <code>text-descriptives</code>.</p> <p> Tutorial</p> </li> <li> <p> RAG: retrievers and reranker</p> <p>In this tutorial, you will learn how to monitor and optimize retrieval and reranking models in a RAG pipeline using <code>haystack</code> and <code>sentence-transformers</code>.</p> <p> Tutorial</p> </li> <li> <p> RAG: LLMs</p> <p>In this tutorial, you will learn how to monitor and optimize generative LLMs in a RAG pipeline using <code>haystack</code> and <code>trl</code>.</p> <p> Tutorial</p> </li> <li> <p> Instruction-tuning an LLM</p> <p>In this tutorial, we will curate data for supervised fine-tuning (SFT) and fine-tune an LLM using <code>trl</code>.</p> <p> Tutorial</p> </li> <li> <p> Preference tuning an LLM</p> <p>In this tutorial, we will curate data for preference alignment of an LLM in order to fine-tune using <code>trl</code> and the DPO algorithm.</p> <p> Tutorial</p> </li> </ul>"},{"location":"tutorials/#leveladvanced","title":"level:advanced","text":"<ul> <li>RAG: Optimizing LLM for generation</li> </ul>"},{"location":"tutorials/#techniquerag","title":"technique:RAG","text":"<ul> <li>RAG: Optimizing LLM for generation</li> </ul>"},{"location":"tutorials/#typegenerative","title":"type:generative","text":"<ul> <li>RAG: Optimizing LLM for generation</li> </ul>"},{"location":"tutorials/rag_llm/","title":"RAG: Optimizing LLM for generation","text":"","tags":["level:advanced","type:generative","technique:RAG"]},{"location":"tutorials/rag_rankers_and_retrievers/","title":"Rag rankers and retrievers","text":"<p>description: This tutorial demonstrates how to create a pipeline for the LLM model using the RAG architecture. tags:   - level:intermediate   - type:predictive   - technique:RAG</p>"},{"location":"tutorials/","title":"Tutorials","text":"<p>These are the tutorials for the Argilla-python SDK. They provide step-by-step instructions for common scenarios, including detailed explanations and code samples.</p> <ul> <li> <p> Few-shot text classification with SetFit</p> <p>In this tutorial, you will learn how to use bulk labeling with semantic search from <code>sentence-transformers</code> embeddings and how to fine-tune a  <code>SetFit</code> model for classification.</p> <p> Tutorial</p> </li> <li> <p> Zero-shot token classification with GliNER</p> <p>In this tutorial, you will learn how to use zero-shot predictions from <code>GliNER</code> to fine-tune a span classification model with <code>SpanMarker</code>.</p> <p> Tutorial</p> </li> <li> <p> Bootstrap projects using LLMs</p> <p>In this tutorial, you will learn how to use LLMs to bootstrap projects for tasks like text classification, token classification, relationship extraction and summarization using <code>spacy-llm</code></p> <p> Tutorial</p> </li> <li> <p> Multi-modal projects</p> <p>In this tutorial, you will learn how to use Argilla for basic multi-modal projects with PDFs, images, video and audio. We will bulk label using <code>sentence-transformers</code> CLIP embeddings and fine-tune a model with <code>transformers</code>.</p> <p> Tutorial</p> </li> <li> <p> Monitor for data and model drift</p> <p>In this tutorial, you will learn you can keep track of model performance and data quality using <code>BerTopic</code> and <code>text-descriptives</code>.</p> <p> Tutorial</p> </li> <li> <p> RAG: retrievers and reranker</p> <p>In this tutorial, you will learn how to monitor and optimize retrieval and reranking models in a RAG pipeline using <code>haystack</code> and <code>sentence-transformers</code>.</p> <p> Tutorial</p> </li> <li> <p> RAG: LLMs</p> <p>In this tutorial, you will learn how to monitor and optimize generative LLMs in a RAG pipeline using <code>haystack</code> and <code>trl</code>.</p> <p> Tutorial</p> </li> <li> <p> Instruction-tuning an LLM</p> <p>In this tutorial, we will curate data for supervised fine-tuning (SFT) and fine-tune an LLM using <code>trl</code>.</p> <p> Tutorial</p> </li> <li> <p> Preference tuning an LLM</p> <p>In this tutorial, we will curate data for preference alignment of an LLM in order to fine-tune using <code>trl</code> and the DPO algorithm.</p> <p> Tutorial</p> </li> </ul>"},{"location":"tutorials/#leveladvanced","title":"level:advanced","text":"<ul> <li>RAG: Optimizing LLM for generation</li> </ul>"},{"location":"tutorials/#techniquerag","title":"technique:RAG","text":"<ul> <li>RAG: Optimizing LLM for generation</li> </ul>"},{"location":"tutorials/#typegenerative","title":"type:generative","text":"<ul> <li>RAG: Optimizing LLM for generation</li> </ul>"}]}